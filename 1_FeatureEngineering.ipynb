{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In machine learning, a feature is an individual measurable attribute of a phenomenon being observed. The extraction of features from raw training data is called *feature engineering*. In other words, feature engineering is a process of transforming raw training data into a *representation* suitable for the application of machine learning algorithms.\n",
    "\n",
    "This process usually requires a certain degree of domain expertise and can be divided into the following stages [[1]](#ref_1): \n",
    "- Brainstorming on features\n",
    "- Deciding what features to create\n",
    "- Creating features\n",
    "- Studying how the features impact model's predictive accuracy\n",
    "- Iterating if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyspark\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession, SQLContext, Window\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pathlib import Path\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "data_dir = './data' #str(Path.home()) + '/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()#.setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "telemetry_input_df = sqlContext.read.parquet(data_dir + '/telemetry')\n",
    "logs_input_df = sqlContext.read.parquet(data_dir + '/logs')\n",
    "\n",
    "telemetry_input_df.printSchema()\n",
    "logs_input_df.printSchema()\n",
    "\n",
    "telemetry_entries_count = telemetry_input_df.count()\n",
    "log_entries_count = logs_input_df.count()\n",
    "\n",
    "print('Total:\\n\\n{0:10d} telemetry entries\\n{1:10d} log entries'.format(\n",
    "    telemetry_entries_count, log_entries_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Let's answer several basic questions about the input data set.\n",
    "\n",
    "### Q: How many unique machines left a telemetry trace?\n",
    "\n",
    "In this example, it is assumed that all machines are identical; hence, no explicit asset inventory is provided. In practice, equipment metadata, which can include make, model, manufacturing date, start date of service, location and other technical specifications, often serves as an important source of information for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_machines_df = telemetry_input_df.select('machineID').distinct()\n",
    "machine_count = distinct_machines_df.count()\n",
    "print('{0} machine(s)'.format(machine_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What time interval is covered by the telemetry records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_input_df.select(\n",
    "    F.min('timestamp').alias('start'), F.max('timestamp').alias('end')\n",
    ").withColumn(\n",
    "    'duration (days)',\n",
    "    F.datediff(F.col('end'), F.col('start'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What types of log events are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_input_df.select('level').groupBy('level').agg(\n",
    "    F.count('level')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log event types follow the common convention:\n",
    "\n",
    "* **INFO**: \n",
    "non-error entries that, among other things, may contain information about scheduled or unscheduled repair procedures\n",
    "* **WARNING**: \n",
    "non-fatal, but potentially harmful or anomalous conditions\n",
    "* **CRITICAL**:\n",
    "unrecoverable conditions requiring an intervention (repair)\n",
    "\n",
    "### Q: What types of CRITICAL events (failures) have been captured in the logs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failures_df = logs_input_df.where(F.col('level') == 'CRITICAL')\n",
    "\n",
    "failures_df.select('code').groupBy('code').agg(\n",
    "    F.count(F.lit(1)).alias('count')\n",
    ").toPandas().set_index('code').plot.pie(\n",
    "    subplots=True, figsize=(5, 5), autopct='%1.1f%%', title='Failure types')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: How many failures per machine have occured?\n",
    "\n",
    "The output is a table showing the distribution of failure counts per machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "machine_failures_df = distinct_machines_df.join(\n",
    "    failures_df, 'machineID', 'left_outer'\n",
    ").groupBy('machineID').agg(F.count('code').alias('count'))\n",
    "\n",
    "mf_pandas = machine_failures_df.toPandas()\n",
    "mf_pandas.columns = ['# Of Machines', 'Failure Count']\n",
    "mf_pandas.groupby('Failure Count').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What are the properties of a single machine's raw telemetry stream?\n",
    "\n",
    "#### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get ID of one of the most \"faulty\" machines\n",
    "faultyMachineID = machine_failures_df.orderBy(F.desc('count')).first().machineID\n",
    "\n",
    "faulty_machine_sequence_df = telemetry_input_df.where(\n",
    "    telemetry_input_df.machineID == faultyMachineID\n",
    ").orderBy(telemetry_input_df.timestamp)\n",
    "\n",
    "faulty_machine_sequence_pdf = faulty_machine_sequence_df.toPandas()\n",
    "faulty_machine_sequence_pdf.set_index('timestamp', inplace=True)\n",
    "print('Descriptive statistics of \"faulty\" machine {}'.format(faultyMachineID))\n",
    "faulty_machine_sequence_pdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparison, here shows the descriptive statistics of a **healthy** machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get ID of one of the \"healthy\" machines\n",
    "healthyMachineID = machine_failures_df.orderBy(F.asc('count')).first().machineID\n",
    "\n",
    "healthy_machine_sequence_df = telemetry_input_df.where(\n",
    "    telemetry_input_df.machineID == healthyMachineID\n",
    ").orderBy(telemetry_input_df.timestamp)\n",
    "\n",
    "healthy_machine_sequence_pdf = healthy_machine_sequence_df.toPandas()\n",
    "healthy_machine_sequence_pdf.set_index('timestamp', inplace=True)\n",
    "print('Descriptive statistics of \"healthy\" machine {}'.format(healthyMachineID))\n",
    "healthy_machine_sequence_pdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data sample\n",
    "print('Data sample of \"faulty\" machine {}'.format(faultyMachineID))\n",
    "faulty_machine_sequence_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Occurence of entries in the \"faulty\" telemetry stream over time\n",
    "\n",
    "faulty_machine_sequence_pdf.apply(lambda x: 1, axis=1).plot(\n",
    "    title='Telemetry stream of \"faulty\" machine {0}'.format(faultyMachineID), style='.', yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Telemetry frequency\n",
    "# ('diffs' are computed by subtracting the previous timestamp\n",
    "# from the current one for each telemetry entry)\n",
    "\n",
    "diffs = faulty_machine_sequence_pdf.index.to_series().diff()\n",
    "mode_interval = diffs.mode()[0] # mode of the time intervals\n",
    "print('Telemetry frequency: {}'.format(mode_interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Other, less common, intervals between telemetry entries (minutes)\n",
    "\n",
    "gaps = diffs[diffs != mode_interval].dt.seconds/60\n",
    "gaps.hist()\n",
    "plt.title('telemetry frequency of \"faulty\" machine {}'.format(faultyMachineID))\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the machine operates in cycles followed by periods of inactivity.\n",
    "\n",
    "### Understanding cycles/patterns of machine activity \n",
    "\n",
    "Many real-world machines operate in cycles. A cycle can be thought of as a temporal period which can describe a certain state of operation of a machine. For example, the operation of a turbo-fan engine on a commercial aircraft can be described succinctly by the cycles: engine operating (aircraft in flight) or engine off (aircraft grounded). In this case the cycles are simply describing engine on and off states but this need not be the case. For a slightly different example, we can imagine the cycles of operation of a printer in an office. We will have cycles describing the printer during a print job and others describing the printer which is on but in stand-by mode (that is not printing). Lastly, we could have machines which operate continuously (think of a machine in a manufacturing plant). It may be possible to extract cycles in this type of operating conditions as well but in the case that this is problematic or not beneficial, a cycle can be defined as an hour of operation for example. In the following paragraph we will explain why it is useful to subdivide our data into cycles in predictive maintenance scenarios.\n",
    "\n",
    "## Feature creation\n",
    "\n",
    "### Cycle-level telemetry aggregation\n",
    "\n",
    "Raw telemetry streams, while essential for real-time monitoring and failure detection, cannot be used directly when building predictive maintenance models. The assumption is that sudden degradation during an operational cycle (for instance, a 2-8 hour flight) is rather unlikely. The common practice is to generate a single aggregate measurement to characterize the health of an asset during each unit of operation, which is commonly expressed in hours, or cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cycles(df, min_gap = 30):\n",
    "    \"\"\"\n",
    "    Detects cycles and compresses them into \n",
    "    aggregate measurements.    \n",
    "    \n",
    "    :param DataFrame df: input Spark DataFrame\n",
    "    :param int min_gap: seconds between cycles (mininum)\n",
    "    \"\"\"\n",
    "    w = Window.partitionBy('machineID').orderBy('timestamp')\n",
    "\n",
    "    # Difference from the previous record or 0 if this is the first one\n",
    "    diff = F.coalesce(\n",
    "        F.unix_timestamp('timestamp') - F.unix_timestamp(\n",
    "            F.lag('timestamp', 1).over(w)\n",
    "        ), F.lit(0))\n",
    "\n",
    "    # 0 if diff <= 30, 1 otherwise\n",
    "    indicator = (diff > min_gap).cast('integer')\n",
    "\n",
    "    subgroup = F.sum(indicator).over(w).alias('cycle')\n",
    "\n",
    "    return df.select(\"*\", subgroup).groupBy('machineID', 'cycle').agg(\n",
    "        F.max('speed_desired').alias('speed_desired_max'),\n",
    "        F.avg('speed').alias('speed_avg'),\n",
    "        F.avg('temperature').alias('temperature_avg'),\n",
    "        F.max('temperature').alias('temperature_max'),\n",
    "        F.avg('pressure').alias('pressure_avg'),\n",
    "        F.max('pressure').alias('pressure_max'),\n",
    "        F.min('timestamp').alias('cycle_start'),\n",
    "        F.max('timestamp').alias('cycle_end')\n",
    "    ).orderBy('cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on the single-machine data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_cycles_df = aggregate_cycles(faulty_machine_sequence_df)\n",
    "machine_cycles_df[['temperature_avg',\n",
    "             'temperature_max',\n",
    "             'pressure_avg',\n",
    "             'pressure_max']].toPandas().plot(subplots=True,\n",
    "                                              title='Cycle-level measurements of \"faulty\" machine {}'.format(faultyMachineID))\n",
    "plt.xlabel('Cycles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparison, here shows the cycle-level measurements of a **healthy** machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_cycles_df = aggregate_cycles(healthy_machine_sequence_df)\n",
    "machine_cycles_df[['temperature_avg',\n",
    "             'temperature_max',\n",
    "             'pressure_avg',\n",
    "             'pressure_max']].toPandas().plot(subplots=True,\n",
    "                                              title='Cycle-level measurements of \"healthy\" machine {}'.format(healthyMachineID))\n",
    "plt.xlabel('Cycles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation of the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles_df = aggregate_cycles(telemetry_input_df)\n",
    "print('Total: {0} cycle(s)'.format(cycles_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "\n",
    "Machine cycles preceding failure events in the logs can be assigned remaining useful life (RUL) labels based on their position in the sequence. For example, the cycle immediately before a failure is assigned RUL equal 1, and the cycle preceding it is assigned RUL equal 2, and so on.\n",
    "\n",
    "RUL is translated into multi-class labels as follows:\n",
    "\n",
    "$$\n",
    "L_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      F_i & rul_i\\leq w \\\\\n",
    "      \\emptyset & \\text{otherwise} \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where $F_i$ is the type of the failure which ends the sequence the entry is a part of, $rul_i$ is the remaining useful life assigned to the entry, and $w$ is the future horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating failure intervals\n",
    "\n",
    "w = Window.partitionBy('machineID', 'level').orderBy('timestamp')\n",
    "# the period between consequtive failures,\n",
    "# or \"since the beginning of time\" if no previous failure record is on file\n",
    "diff = F.coalesce(F.lag('timestamp', 1).over(w), F.to_timestamp(F.lit('2000-01-01 00:00:00')))\n",
    "\n",
    "failure_intervals_df = (failures_df\n",
    "                .withColumn('last_failure_timestamp', diff)\n",
    "                .withColumnRenamed('timestamp', 'failure_timestamp')\n",
    "                .withColumnRenamed('code', 'upcoming_failure')\n",
    "                .drop('level'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure intervals for the selected machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_failure_intervals_df = failure_intervals_df.where(\n",
    "    F.col('machineID') == faultyMachineID)\n",
    "machine_failure_intervals_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label assignment for all cycles\n",
    "\n",
    "In addition, for each sequence of cycles running up to a failure, as well as those not ending in a failure, a unique sequence ID is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 7 # future horizon (cycles)\n",
    "\n",
    "labeled_cycles_df = (\n",
    "    cycles_df.join(failure_intervals_df,\n",
    "                   (cycles_df.machineID == failure_intervals_df.machineID) &\n",
    "                   (cycles_df.cycle_start >= failure_intervals_df.last_failure_timestamp) &\n",
    "                   (cycles_df.cycle_end <= failure_intervals_df.failure_timestamp),\n",
    "                   'left_outer')\n",
    "    .drop(failure_intervals_df.machineID)\n",
    "    .drop(cycles_df.cycle_end)\n",
    "    .drop(failure_intervals_df.last_failure_timestamp)\n",
    "    .withColumnRenamed('cycle_start', 'timestamp')\n",
    "    .withColumn(\n",
    "     'rul',                         \n",
    "      F.when(F.col('upcoming_failure').isNull(), None).otherwise(\n",
    "         F.row_number().over(Window.partitionBy('machineID', 'failure_timestamp')\n",
    "                         .orderBy(F.desc('cycle')))))\n",
    "    .withColumn(\n",
    "     'sequenceID',\n",
    "     F.dense_rank().over(Window.partitionBy('machineID')\n",
    "                   .orderBy(F.desc('upcoming_failure'), 'failure_timestamp')))\n",
    "    .drop(failure_intervals_df.failure_timestamp)\n",
    "    .withColumn('immediate_failure',\n",
    "                F.when(F.col('rul').isNotNull() & (F.col('rul') < w),\n",
    "                       F.col('upcoming_failure'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure intervals and sequences for the selected machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_labeled_cycles_df = labeled_cycles_df.where(\n",
    "    F.col('machineID') == faultyMachineID\n",
    ").select('timestamp', 'sequenceID', 'upcoming_failure')\n",
    "\n",
    "max_sequence_id = machine_labeled_cycles_df.select(\n",
    "    F.max('sequenceID')).collect()[0][0]\n",
    "machine_labeled_cycles_df.toPandas().set_index('timestamp').plot(\n",
    "    style='.', yticks=range(1, max_sequence_id + 1))\n",
    "plt.ylabel('Sequence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled data set\n",
    "labeled_cycles_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting data with lag features\n",
    "\n",
    "To help characterize short-term trends for each machine, it is common practice [[3]](#ref_3) to augment the data set with new features computed over rolling windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute more rolling features\n",
    "# (e.g., linear regression, standard deviation)\n",
    "\n",
    "lookback = 5\n",
    "w = Window.partitionBy(\n",
    "    'machineID', 'sequenceID'\n",
    ").rowsBetween(-lookback, Window.currentRow).orderBy('cycle')\n",
    "\n",
    "# computing rolling averages for these cycle-level features\n",
    "input_columns = ['temperature_avg', 'temperature_max', 'pressure_avg', 'pressure_max']\n",
    "\n",
    "# adding rolling features and eliminating the entries in the beginning of each\n",
    "# sequence which weren't computed using a sufficient number of previous values\n",
    "augmented_labeled_cycles_df = (\n",
    "    reduce(lambda _df, ic: _df.withColumn(\n",
    "        '{0}_avg'.format(ic), F.avg(ic).over(w)), input_columns, labeled_cycles_df)\n",
    "    .withColumn('is_valid', F.count(F.lit(1)).over(w) > lookback)\n",
    "    .where(F.col('is_valid'))\n",
    "    .drop('is_valid'))\n",
    "\n",
    "augmented_labeled_cycles_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the feature data set\n",
    "\n",
    "For the sake of generality, all domain-specific feature names will be obfuscated, and explicit timestamps will be replaced with numeric entry IDs indicating the relative temporal order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    c for c in augmented_labeled_cycles_df.columns if c not in (\n",
    "        'machineID', 'cycle', 'sequenceID', 'rul', 'upcoming_failure', 'immediate_failure', 'timestamp')]\n",
    "obfuscate_columns = zip(feature_columns, range(1, len(feature_columns) + 1))\n",
    "\n",
    "feature_df = reduce(lambda _df, ic: _df.withColumnRenamed(ic[0], 's{0}'.format(ic[1])),\n",
    "    obfuscate_columns, augmented_labeled_cycles_df).withColumn(\n",
    "    'entryID', F.row_number().over(Window().orderBy('timestamp'))\n",
    ").drop('upcoming_failure', 'timestamp')\n",
    "\n",
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = 5 # split the data into N chunks\n",
    "feature_df.coalesce(chunks).write.csv(data_dir + '/features',mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://www.youtube.com/watch?v=drUToKxEAUA\n",
    "\n",
    "http://waset.org/publications/10006640/building-a-scalable-telemetry-based-multiclass-predictive-maintenance-model-in-r\n",
    "\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.386.8108&rep=rep1&type=pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
