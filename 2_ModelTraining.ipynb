{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this notebooks, we formulate a multi-class classification problem as follows:\n",
    "\n",
    "> Is a machine going to need maintenance within the next N cycles, and if yes, due to what type of a failure?\n",
    "\n",
    "First, we define the future horizon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "data_dir = './data'#str(Path.home()) + '/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob(data_dir + '/features/*.csv')\n",
    "converters={\"failure\": str}\n",
    "seed = 42\n",
    "dfs= [pd.read_csv(filename, converters=converters) for filename in filenames]\n",
    "data = pd.concat(dfs, ignore_index=True).fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "Two split strategies are implemented below:\n",
    "* time-dependent split\n",
    "* asset ID-based split\n",
    "\n",
    "Time-dependent split is more complex as it requires that training and test data sets don't have common rolling feature time frames. This means that for all sequences in the test data set, $X$ immediately preceding entries need to be removed from the training data, where $X$ is the length of the rolling aggregation window used during feature engineering minus 1 [[2]](#ref_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 5       # rolling aggregation interval used during feature engineering\n",
    "test_size = 0.2\n",
    "time_split = False  # if set to False, will perform asset ID-based split\n",
    "\n",
    "if time_split:\n",
    "    data.set_index(['entryID'], inplace=True)\n",
    "    data.sort_index(inplace=True)\n",
    "\n",
    "    train, test = train_test_split(data, test_size=test_size, shuffle=False)\n",
    "    \n",
    "    min_cycles = test.reset_index().groupby(\n",
    "        ['machineID']\n",
    "    ).cycle.min().apply(lambda x: x - lookback).to_frame(name='max_cycle')\n",
    "    \n",
    "    t = train.reset_index().join(min_cycles, on='machineID')\n",
    "    train = t[t.max_cycle.isna() |\n",
    "              (t.cycle < t.max_cycle)].drop('max_cycle', axis=1)\n",
    "    train.set_index(['entryID'], inplace=True)\n",
    "else:\n",
    "    # asset ID-based split\n",
    "    unique_assets = data.reset_index().machineID.unique()\n",
    "    train_assets, test_assets = train_test_split(\n",
    "        unique_assets, test_size=test_size, random_state=seed)\n",
    "    train = data[data.machineID.isin(train_assets)]\n",
    "    test = data[data.machineID.isin(test_assets)]\n",
    "    train.set_index(['entryID'], inplace=True)\n",
    "    test.set_index(['entryID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_split(data):\n",
    "    data = data.reset_index(drop = True)\n",
    "    columns_to_drop = ['cycle', 'immediate_failure', 'rul', 'sequenceID', 'machineID']\n",
    "    return (data.drop(columns_to_drop, axis=1),\n",
    "            data['immediate_failure'])\n",
    "\n",
    "X_train, Y_train = xy_split(train)\n",
    "X_test, Y_test = xy_split(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction of data imbalance\n",
    "\n",
    "In typical predictive maintenance data sets, positive examples as often underrepresented relative to negative examples. This can be seen by counting failure types in the \"ground truth\" training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = Counter(Y_train)\n",
    "majority_class = all_classes.most_common(1)\n",
    "minority_classes = all_classes.most_common()[1:]\n",
    "\n",
    "print('Majority class: ', majority_class)\n",
    "print('Minority classes: ', minority_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With class imbalance in data, performance of most standard learning algorithms is compromised, since they aim to minimize the overall error rate. For a data set with 99% negative and 1% positive examples, a model can be shown to have 99% accuracy by labeling all instances as negative. But the model will mis-classify all positive examples; so even if its accuracy is high, the algorithm is not a useful one.\n",
    "\n",
    "Here, we will use the Synthetic Minority Over-sampling Technique (SMOTE) [[3]](#ref_3) to produce a more balanced training data set with at least 10% of positive examples. Note that over-sampling is not applied to the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_classes_size = sum([c[1] for c in minority_classes])\n",
    "desired_minority_classes_size = Y_train.count() * 0.1\n",
    "\n",
    "scale = desired_minority_classes_size / minority_classes_size\n",
    "\n",
    "ratio = None\n",
    "if scale > 1:\n",
    "    ratio = dict((c[0], int(c[1] * scale)) for c in minority_classes)\n",
    "\n",
    "sm = SMOTE(ratio=ratio, random_state=seed)\n",
    "X_train_res, Y_train_res = sm.fit_sample(X_train, Y_train)\n",
    "Counter(Y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Decision Trees are among the most popular and versatile classification methods. They work with both numerical and categorical data, and perform well even given relatively small training data sets.\n",
    "\n",
    "Using decision trees within an ensemble (called Random Decision Forest) allows alleviating several problems:\n",
    "* overfitting\n",
    "* multi-collinearity\n",
    "\n",
    "### Fitting the Random Forest classifier on the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=seed)\n",
    "clf.fit(X_train_res, Y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "### Confusion matrix, precision, recall and F1 score\n",
    "\n",
    "The easiest to visualize and interpret summary of a multi-class classifier's performance is the confusion matrix. This matrix is a juxtaposition of the classifier's predictions against the ground truth categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions = clf.predict(X_test)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    orig = cm\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt) + '\\n({0})'.format(orig[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#cm = confusion_matrix(Y_test, binarizer.inverse_transform(Y_predictions))\n",
    "cm = confusion_matrix(Y_test, Y_predictions)\n",
    "plot_confusion_matrix(cm, ['None'] + [c[0] for c in minority_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification tasks, most of the classifier's performance measures can be derived directly from the entries of the standard two-by-two confusion matrix.\n",
    "\n",
    "$$\n",
    "\\begin{matrix} \n",
    " & Prediction=Negative & Prediction=Positive \\\\\n",
    "Actual=Negative & \\scriptsize True\\ negatives\\ (TN) & \\scriptsize False\\ positives (FP) \\\\\n",
    "Actual=Positive & \\scriptsize False\\ netagives\\ (FN) & \\scriptsize True\\ positives\\ (TP) \n",
    "\\end{matrix}\n",
    "$$ \n",
    "\n",
    "Here are the definitions of several most common model performance measures:\n",
    "\n",
    "| Measure | Formula |\n",
    "|:--- |------|\n",
    "| Precision |   $\\frac{TP}{TP+FP}$  |\n",
    "| Recall | $\\frac{TP}{TP+FN}$ |\n",
    "| F1 score | $\\frac{2\\cdot precision\\cdot recall}{precision+recall}$ |\n",
    "\n",
    "In a multi-class context, these measures are computed for each label independently and then averaged across the entire set of classes (as demonstrated in the classification report below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: ```classification_report``` function computes the averages taking class imbalance into account; for that reason, they are heavily biased towards the majority class.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "In general, *accuracy* is an inappropriate measure for unbalanced classes. To demonstrate that, let's compare the accuracy of our model against that of a dummy classifier (sometimes called a *null* model) which always returns the most frequent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train_res, Y_train_res)\n",
    "Y_dummy = dummy.predict(X_test)\n",
    "\n",
    "print('Accuracy scores')\n",
    "print('Trained model: {0}\\nDummy classifier: {1}'.format(accuracy_score(Y_test, Y_predictions),\n",
    "                                                         accuracy_score(Y_test, Y_dummy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With respect to accuracy, the trained model only slightly outperforms a dummy classifier.\n",
    "\n",
    "### Area Under the Curve (AUC)\n",
    "\n",
    "AUC is the area under the *receiver operating characteristic curve* (ROC curve), which is 1.0 for ideal classifiers and 0.5 for those that do no better than random guessing. Let's compare the AUC score of the trained model with that of the dummy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score expects binarized labels\n",
    "binarizer = LabelBinarizer()\n",
    "binarizer.fit(Y_train_res)\n",
    "Y_test_binarized = binarizer.transform(Y_test)\n",
    "\n",
    "def auc_score(y_true, y_pred):\n",
    "    return roc_auc_score(binarizer.transform(y_true), binarizer.transform(y_pred), average='macro')\n",
    "\n",
    "print('ROC AUC scores')\n",
    "print('Trained model: {0}\\nDummy classifier: {1}'.format(auc_score(Y_test, Y_predictions),\n",
    "                                                         auc_score(Y_test, Y_dummy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC score would be good candidate when a single sensitive model evaluation measure is needed.\n",
    "\n",
    "## Persisting the model and input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf, 'model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X_test.sample(n = 5).to_json(orient='records')\n",
    "print('Sample:', sample)\n",
    "\n",
    "with open('sample.json', 'w') as sample_file:\n",
    "    sample_file.write(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/cortana-analytics-playbook-predictive-maintenance\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/cortana-analytics-playbook-predictive-maintenance#time-dependent-split\n",
    "\n",
    "https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume16/chawla02a-html/chawla2002.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
